y<-10:15
cbind(x,y)
rbind(x,y)
x<-(1,"a",1+0i,true)
x<-list(1,"a",true,1+4i)
x<-list(1,"a",TRUE,1+4i)
X
x
LM()
lm()
glm()}
x<-factor(c("yes","no","yes")
x
x<-factor(c("yes","yes","no"))
x
table(x)
unclass(x)
y<-data.frame(a="1",b="2")
dput(y)
dput(y,file="y.r")
new.y<-dget("y.r")
new.y
save.image("H:\\Docs\\Studying\\Personal Project\\BigData\\Data Scientist_John Hopkins\\R_1")
con <- url("http://www.jhsph.edu", "r")
x <- readLines(con)
head(x)
save.image("H:\\Docs\\Studying\\Personal Project\\BigData\\Data Scientist_John Hopkins\\R_2")
q()
add2<-function(x,y){
x+y
}
c<-add2(3,4)
c
x<-1:20
x
above<-function(y,n){}
above<-function(y,n){
use<-x>n
print(x[use])
}
above(x,19)
above(x,20)
above(x,21)
above(x,-1)
above(x,0)
above(x,-2)
columnmean<-function(y,removeNA=TRUE){
nc<-ncol(y)
means<-numeric(nc)
for(i in 1:nc){
means[i]<-mean(y,[,i])
}
means
}
columnmean<-function(y,removeNA=TRUE){
nc<-ncol(y)
means<-numeric(nc)
for(i in 1:nc){
means[i]<-mean(y,y[,i])
}
means
}
z<-columnmean(airquality)
function(columnean)
end
function(columnmean)
ens
columnmean<-function(x,removeNA=TRUE){}
columnmean<-function(x,removeNA=TRUE){}
columnmean<-function(x,removeNA=TRUE){}
columnmean<-function(x,removeNA=TRUE){
nc<-ncol(x)
means<-numeric(nc)
for(i in 1:nc){}
for(i in 1:nc){means[i]=mean(x[,i],na.rm=removeNA)
}
means
}
z<-columnmean(airquality)
z
z<-columnmean(airquality,removeNA=FALSE)
Z
z
q()
x<-sys.date()
x<-sys.time()
x<-Sys.time()
x
cube<-function(x,n){}
cube<-function(x,n){}
cube<-function(x,n){ }
cube<-function(x,n){   }
cube<-function(x,n){   }
cube<-function(x,n){   }
cube<-function(x,n){
x^3
}
cube(3)
x<-1:10
if(x>5){x<-0 }
if(x>5){}
x<-1:10
if(x>5){
x<-0
}
f<-function(x){}
f<-function(x){
g<-function(y){
y+z
}
z<-4
x+g(x)
}
z<-10
f(3)
q()
makeVector <- function(x = numeric()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setmean <- function(mean) m <<- mean
getmean <- function() m
list(set = set, get = get,
setmean = setmean,
getmean = getmean)
}
dim(m)=c(4,4)
dim(v)=c(4,4)
m=rnorm(16)
dim(m)
dim(m)=c(4,4)
x=makevector(m)
x=makeVector(m)
x
makevector
makeVector
m=rnorm(16)
dim(m)=c(4,4)
x=makeCachematrix(m)
## Cache the inverse of a matrix
## Function makeCacheMatrix creates a datastructure
## which stores the inverse of a matrix passed
## Function cacheSolve utlizes the matrix created by makeCacheMatrix
## to actually store the inverse of the mtrix if it has not been done by the function.
## This function creates a data structure which stores the inverse of a matrix
makeCacheMatrix <- function(x = matrix()) {
i<-null
set<-function(y){
x<<-y
i<<-null
}
get<-function()x
setinverse<-function(inverse)i<<-inverse
getinverse<-function()i
list(set=set,get=get
setinverse=setinverse,
getinverse=getinverse)
}
## This function stores the inverse of the matrix if it is not present
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
i<-x$getinverse()
if(!is.null(i)){
print("getting the cached version of inverse")
return(i)
}
mat<-x$get()
i<-solve(mat)
x$stinverse(i)
i
}
## Cache the inverse of a matrix
## Function makeCacheMatrix creates a datastructure
## which stores the inverse of a matrix passed
## Function cacheSolve utlizes the matrix created by makeCacheMatrix
## to actually store the inverse of the mtrix if it has not been done by the function.
## This function creates a data structure which stores the inverse of a matrix
makeCacheMatrix <- function(x = matrix()) {
i<-null
set<-function(y){
x<<-y
i<<-null
}
get<-function()x
setinverse<-function(inverse)i<<-inverse
getinverse<-function()i
list(set=set,get=get
setinverse=setinverse,
getinverse=getinverse)
}
## This function stores the inverse of the matrix if it is not present
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
i<-x$getinverse()
if(!is.null(i)){
print("getting the cached version of inverse")
return(i)
}
mat<-x$get()
i<-solve(mat)
x$stinverse(i)
i
}
## Cache the inverse of a matrix
## Function makeCacheMatrix creates a datastructure
## which stores the inverse of a matrix passed
## Function cacheSolve utlizes the matrix created by makeCacheMatrix
## to actually store the inverse of the mtrix if it has not been done by the function.
## This function creates a data structure which stores the inverse of a matrix
makeCacheMatrix <- function(x = matrix()) {
i<-null
set<-function(y){
x<<-y
i<<-null
}
get<-function()x
setinverse<-function(inverse)i<<-inverse
getinverse<-function()i
list(set=set,get=get,
setinverse=setinverse,
getinverse=getinverse)
}
## This function stores the inverse of the matrix if it is not present
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
i<-x$getinverse()
if(!is.null(i)){
print("getting the cached version of inverse")
return(i)
}
mat<-x$get()
i<-solve(mat)
x$stinverse(i)
i
}
m=rnorm(16)
dim(m)=c(4,4)
x=makeCacheMatrix(m)
## Cache the inverse of a matrix
## Function makeCacheMatrix creates a datastructure
## which stores the inverse of a matrix passed
## Function cacheSolve utlizes the matrix created by makeCacheMatrix
## to actually store the inverse of the matrix if it has not been done by the function.
## This function creates a data structure which stores the inverse of a matrix
makeCacheMatrix <- function(x = matrix()) {
i<-NULL
set<-function(y){
x<<-y
i<<-NULL
}
get<-function()x
setinverse<-function(inverse)i<<-inverse
getinverse<-function()i
list(set=set,get=get,
setinverse=setinverse,
getinverse=getinverse)
}
## This function stores the inverse of the matrix if it is not present
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
i<-x$getinverse()
if(!is.null(i)){
print("getting the cached version of inverse")
return(i)
}
mat<-x$get()
i<-solve(mat)
x$setinverse(i)
i
}
m=rnorm(16)
dim(m)=c(4,4)
x=makeCacheMatrix(m)
CacheSolve(x)
cacheSolve(x)
cacheSolve(x)
cacheSolve(x)
m=rnorm(9)
dim(m)=c(3,3)
x=makeCacheMatrix(m)
cacheSolve(x)
cacheSolve(x)
q()
library(datasets)
data(iris)
?irirs
?iris
apply(iris[,1],2,mean)
apply(iris[,1:4],2,mean)
apply(iris[,1:5],2,mean)
apply(iris3[,1:4,3],2,mean)
apply(iris3[,1:5,3],2,mean)
?mtcars
mtcars[,2]
let avg4cyl = mtcars.Data |> Seq.filter (fun x -> x?cyl = "4") |> Seq.averageBy (fun x -> x?hp.AsFloat())
avg4cyl = mtcars.Data |> Seq.filter (fun x -> x?cyl = "4") |> Seq.averageBy (fun x -> x?hp.AsFloat())
library(datasets)
data(mtcars)
tapply(mtcars[,1],mtcars[,2],mean)
abs(mean(mtcars[,2]=4,$hp)-mean(mtcars[,2]=8,$hp))
abs(mean([mtcars$cyl==4]$hp)-mean([mtcars$cyl==8]$hp))
abs(mean([mtcars$cyl==4,]$hp)-mean([mtcars$cyl==8,]$hp))
abs(mean(mtcars[mtcars$cyl==4,]$hp)-mean(mtcars[mtcars$cyl==8,]$hp))
debug(ls)
ls
debug(ls)
ls
n
q()
r.version.string
R.version.string
install.packages("swirl")
library(swirl)
swirl()
INFO
0
swirl()
5+7
X<-5+7
X <- 5+7
x <- 5+7
x
y <- x-3
y
z <- c(1.1,9,3.14)
?c
z
c(z,555)
c(z,555,z)
z*2+100
my_sqrt <- sqrt(z-1)
my_sqrt
my_div <- z/my_sqrt
my_div
v <- c(1,2,3,4)+c(0,10)
c(1,2,3,4)+c(0,10)
c(1,2,3,4)+c(0,10,100)
z*2+1000
my_div
1:20
pi:10
15:1
?`:`
seq(1,20)
seq(1,20,by=0.5)
seq(0,10,by=0.5)
seq(5,10,length=30)
my_seq <- seq(5,10,length=30)
length(my_seq)
1:length(my_seq)
seq(along.with=my_seq)
seq_along(my_seq)
rep(0,times=40)
rep(c(0,1,2),times=10)
rep(c(0,1,2),each=10)
num_vect <- c(0.5,55,-10,6)
tf <- num_vect<1
tf
num_vect >= 6
my_char <- c("my","name","is")
my_char <- c("My","name","is")
my_char
paste(my_char,collapse=" ")
my_name <- c(my_char,"VIVEK C P")
my_name
paste(my_name,collapse=" ")
paste("Hello","world",sep=" ")
paste("Hello","world!",sep=" ")
paste(1:3,c("X","Y","Z"),sep="")
paste(LETTERS,1:4,sep="_")
paste(LETTERS,1:4,sep="-")
x <- c(44,NA,5,NA)
X*3
x*3
y <- rnorm(1000)
z <- rep(NA,1000)
my_data <- sample(c(y,z),100)
my_na <- is.na(my_data)
my_data
my_na
my_data==NA
sum(my_na)
my_data
0/0
Inf-Inf
x <- c(rnorm(20),rep(NA,times=20))
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y>0]
x[x>0]
x[!is.na(x) & x>0]
c(x[3],x[5],x[7])
x[c(3,5,7)]
x[0]
x[3000]
x[c(-2,-10)]
x[-c(2,10)]
vect <- c(foo=11,bar=2,norf=NA)
vect
names(vect)
vect2 <- c(11,2,NA)
names(vect2) <- c("foo","bar","norf")
identical(vect,vect2)
vect["bar"]
vect[c("foo","bar")]
bye()
q()
setwd("C:/Users/VIVEK C P/Desktop/R/Getting & Cleaning Data/Course Project")
## Create one R script called run_analysis.R that does the following:
## 1. Merges the training and the test sets to create one data set.
## 2. Extracts only the measurements on the mean and standard deviation for each measurement.
## 3. Uses descriptive activity names to name the activities in the data set
## 4. Appropriately labels the data set with descriptive activity names.
## 5. Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
if (!require("data.table")) {
install.packages("data.table")
}
if (!require("reshape2")) {
install.packages("reshape2")
}
require("data.table")
require("reshape2")
# Load: activity labels
activity_labels <- read.table("./UCI HAR Dataset/activity_labels.txt")[,2]
# Load: data column names
features <- read.table("./UCI HAR Dataset/features.txt")[,2]
# Extract only the measurements on the mean and standard deviation for each measurement.
extract_features <- grepl("mean|std", features)
# Load and process X_test & y_test data.
X_test <- read.table("./UCI HAR Dataset/test/X_test.txt")
y_test <- read.table("./UCI HAR Dataset/test/y_test.txt")
subject_test <- read.table("./UCI HAR Dataset/test/subject_test.txt")
names(X_test) = features
# Extract only the measurements on the mean and standard deviation for each measurement.
X_test = X_test[,extract_features]
# Load activity labels
y_test[,2] = activity_labels[y_test[,1]]
names(y_test) = c("Activity_ID", "Activity_Label")
names(subject_test) = "subject"
# Bind data
test_data <- cbind(as.data.table(subject_test), y_test, X_test)
# Load and process X_train & y_train data.
X_train <- read.table("./UCI HAR Dataset/train/X_train.txt")
y_train <- read.table("./UCI HAR Dataset/train/y_train.txt")
subject_train <- read.table("./UCI HAR Dataset/train/subject_train.txt")
names(X_train) = features
# Extract only the measurements on the mean and standard deviation for each measurement.
X_train = X_train[,extract_features]
# Load activity data
y_train[,2] = activity_labels[y_train[,1]]
names(y_train) = c("Activity_ID", "Activity_Label")
names(subject_train) = "subject"
# Bind data
train_data <- cbind(as.data.table(subject_train), y_train, X_train)
# Merge test and train data
data = rbind(test_data, train_data)
id_labels   = c("subject", "Activity_ID", "Activity_Label")
data_labels = setdiff(colnames(data), id_labels)
melt_data      = melt(data, id = id_labels, measure.vars = data_labels)
# Apply mean function to dataset using dcast function
tidy_data   = dcast(melt_data, subject + Activity_Label ~ variable, mean)
write.table(tidy_data, file = "./tidy_data.txt")
---
---
title: "CodeBook"
author: "Vivek C P"
date: "Monday, October 27, 2014"
output: word_document
---
## CodeBook
This is a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data.
## The data source
* Original data: https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
* Original description of the dataset: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
## Data Set Information
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.
The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.
## The data
The dataset includes the following files:
* 'README.txt'
* 'features_info.txt': Shows information about the variables used on the feature vector.
* 'features.txt': List of all features.
* 'activity_labels.txt': Links the class labels with their activity name.
* 'train/X_train.txt': Training set.
* 'train/y_train.txt': Training labels.
* 'test/X_test.txt': Test set.
* 'test/y_test.txt': Test labels.
The following files are available for the train and test data. Their descriptions are equivalent.
* 'train/subject_train.txt': Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30.
* 'train/Inertial Signals/total_acc_x_train.txt': The acceleration signal from the smartphone accelerometer X axis in standard gravity units 'g'. Every row shows a 128 element vector. The same description applies for the 'total_acc_x_train.txt' and 'total_acc_z_train.txt' files for the Y and Z axis.
* 'train/Inertial Signals/body_acc_x_train.txt': The body acceleration signal obtained by subtracting the gravity from the total acceleration.
* 'train/Inertial Signals/body_gyro_x_train.txt': The angular velocity vector measured by the gyroscope for each window sample. The units are radians/second.
## Transformation details
There are 5 parts:
* Merges the training and the test sets to create one data set.
* Extracts only the measurements on the mean and standard deviation for each           measurement.
* Uses descriptive activity names to name the activities in the data set
* Appropriately labels the data set with descriptive activity names.
* Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
## How run_analysis.R implements the above steps:
* Require reshapre2 and data.table librareis.
* Load both test and train data
* Load the features and activity labels.
* Extract the mean and standard deviation column names and data.
* Process the data. There are two parts processing test and train data respectively.
* Merge data set.
